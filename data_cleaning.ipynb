# %% [markdown]
# # AI-Powered Clinical Data Cleaning System
# ## Clinical Data Management (CDM) Automation Project
# 
# **Objective**: Automatically detect and correct errors in clinical trial data using AI/ML
# 
# ---

# %% [markdown]
# ## Phase 1: Setup and Synthetic Data Generation

# %%
import pandas as pd
import numpy as np
from faker import Faker
import random
from datetime import datetime, timedelta

# Initialize Faker
fake = Faker()

# Create synthetic clinical trial data with intentional errors
def generate_synthetic_clinical_data(n=1000):
    data = []
    for _ in range(n):
        record = {
            'patient_id': fake.uuid4(),
            'age': random.randint(18, 90),
            'gender': random.choice(['M', 'F', 'Male', 'Female', 'U']),  # Inconsistency
            'diagnosis': random.choice(['Diabetes', 'HTN', 'Asthma', 'Cancer', '']),  # Missing
            'systolic_bp': random.gauss(120, 25),
            'diastolic_bp': random.gauss(80, 15),
            'visit_date': (datetime.now() - timedelta(days=random.randint(0,365))).strftime('%Y-%m-%d'),
            'medication': random.choice(['Metformin', 'Lisinopril', 'Albuterol', 'Chemo', 'UNKNOWN']),
            'ae_text': random.choice([
                "Headache", 
                "Nausea and vomiting", 
                "",  # Missing
                "Severe allergic reaction",
                "Incorrectly formatted entry 123"
            ])
        }
        data.append(record)
    
    df = pd.DataFrame(data)
    
    # Introduce systematic errors
    df.loc[df.sample(frac=0.1).index, 'systolic_bp'] = 300  # Impossible values
    df.loc[df.sample(frac=0.1).index, 'diastolic_bp'] = 0   # Impossible values
    df.loc[df.sample(frac=0.15).index, 'age'] = np.nan      # Missing data
    
    return df

clinical_df = generate_synthetic_clinical_data()
clinical_df.head()

# %% [markdown]
# ## Phase 2: Exploratory Data Analysis (EDA)

# %%
import matplotlib.pyplot as plt
import seaborn as sns

# Missing data analysis
plt.figure(figsize=(10,6))
sns.heatmap(clinical_df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Data Heatmap')

# Numerical outliers
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
sns.boxplot(x=clinical_df['systolic_bp'])
plt.title('Systolic BP Distribution')

plt.subplot(1,2,2)
sns.boxplot(x=clinical_df['diastolic_bp'])
plt.title('Diastolic BP Distribution')

# Categorical inconsistencies
print("\nGender value counts:")
print(clinical_df['gender'].value_counts())

print("\nDiagnosis value counts:")
print(clinical_df['diagnosis'].value_counts())

# %% [markdown]
# ## Phase 3: Data Preprocessing Pipeline

# %%
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import FunctionTransformer

# Custom function for text cleaning
def clean_text(text):
    if pd.isna(text):
        return text
    text = text.strip().lower()
    if text.isnumeric():
        return np.nan  # Flag numeric entries in text fields
    return text

# Build preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='median'), ['age', 'systolic_bp', 'diastolic_bp']),
        ('cat', SimpleImputer(strategy='most_frequent'), ['gender', 'diagnosis']),
        ('text', FunctionTransformer(clean_text), ['ae_text'])
    ])

cleaned_data = preprocessor.fit_transform(clinical_df)
cleaned_df = pd.DataFrame(cleaned_data, columns=clinical_df.columns)

# Standardize categorical values
cleaned_df['gender'] = cleaned_df['gender'].map({
    'M': 'Male',
    'F': 'Female',
    'Male': 'Male',
    'Female': 'Female'
}).fillna('Unknown')

# %% [markdown]
# ## Phase 4: Anomaly Detection with Machine Learning

# %%
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor

# Prepare features for anomaly detection
features = cleaned_df[['age', 'systolic_bp', 'diastolic_bp']].dropna()

# Train Isolation Forest model
clf = IsolationForest(contamination=0.1, random_state=42)
clf.fit(features)
anomaly_scores = clf.decision_function(features)

# Add predictions to dataframe
cleaned_df['anomaly_score'] = np.nan
cleaned_df.loc[features.index, 'anomaly_score'] = anomaly_scores
cleaned_df['is_anomaly'] = cleaned_df['anomaly_score'] < np.quantile(anomaly_scores, 0.1)

# Visualize anomalies
plt.figure(figsize=(10,6))
sns.scatterplot(
    x='systolic_bp', 
    y='diastolic_bp', 
    hue='is_anomaly', 
    data=cleaned_df,
    palette={True: 'red', False: 'green'}
)
plt.title('Blood Pressure Anomaly Detection')

# %% [markdown]
# ## Phase 5: Natural Language Processing for AE Text

# %%
import spacy
from spacy.lang.en.stop_words import STOP_WORDS

# Load clinical NLP model
nlp = spacy.load("en_core_sci_lg")

def analyze_ae_text(text):
    if pd.isna(text) or text.strip() == "":
        return np.nan
    
    doc = nlp(text)
    
    # Extract medical concepts
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    
    # Check for invalid formats
    if any(token.like_num for token in doc):
        return "INVALID_FORMAT"
    
    return entities

# Apply NLP analysis
cleaned_df['ae_analysis'] = cleaned_df['ae_text'].apply(analyze_ae_text)

# Show results
print(cleaned_df[['ae_text', 'ae_analysis']].head(10))

# %% [markdown]
# ## Phase 6: Business Rules Engine

# %%
def clinical_validation(row):
    errors = []
    
    # Range checks
    if not (18 <= row['age'] <= 120):
        errors.append(f"Invalid age: {row['age']}")
    
    if not (70 <= row['systolic_bp'] <= 250):
        errors.append(f"Invalid SBP: {row['systolic_bp']}")
    
    if not (40 <= row['diastolic_bp'] <= 150):
        errors.append(f"Invalid DBP: {row['diastolic_bp']}")
    
    # Consistency checks
    if row['systolic_bp'] < row['diastolic_bp']:
        errors.append("SBP < DBP")
    
    # Categorical checks
    if row['diagnosis'] == '':
        errors.append("Missing diagnosis")
    
    return errors if errors else None

# Apply validation
cleaned_df['validation_errors'] = cleaned_df.apply(clinical_validation, axis=1)

# Generate quality report
error_stats = {
    'Total Records': len(cleaned_df),
    'Records with Errors': cleaned_df['validation_errors'].notna().sum(),
    'Common Error Types': pd.Series(
        [err for sublist in cleaned_df['validation_errors'].dropna() for err in sublist]
    ).value_counts().head(5)
}

print("\nData Quality Report:")
print(pd.DataFrame.from_dict(error_stats, orient='index'))

# %% [markdown]
# ## Phase 7: Interactive Dashboard (Streamlit App)

# %%
# Save this part as app.py and run with: streamlit run app.py

"""
import streamlit as st
import pandas as pd

st.title('Clinical Data Quality Dashboard')

uploaded_file = st.file_uploader("Upload Clinical Trial Data", type=['csv'])

if uploaded_file:
    raw_df = pd.read_csv(uploaded_file)
    processed_df = clean_and_analyze(raw_df)  # Reuse functions from notebook
    
    st.subheader("Data Quality Metrics")
    col1, col2, col3 = st.columns(3)
    col1.metric("Total Records", len(processed_df))
    col2.metric("Records with Errors", processed_df['validation_errors'].notna().sum())
    col3.metric("Error Rate", 
               f"{(processed_df['validation_errors'].notna().sum()/len(processed_df)*100:.1f}%")
    
    st.subheader("Anomaly Detection")
    st.scatter_chart(processed_df, x='systolic_bp', y='diastolic_bp', color='is_anomaly')
    
    st.subheader("Error Details")
    st.dataframe(processed_df[processed_df['validation_errors'].notna()].head())
"""
